
%include "x86inc.asm"

extern pshuffq_zero
extern pshuffd_zero
extern pshuffd_w

SECTION .rodata align=16
	dst_partial_bufferfly_4_t4_1		DW		29,  55, 29,  55, 29,  55, 29,  55
										DW		74,   0, 74,   0, 74,   0, 74,   0
										DW		29,  55, 29,  55, 29,  55, 29,  55
										DW		55, -29, 55, -29, 55, -29, 55, -29

	dst_partial_bufferfly_4_t4_2		DW		 29,  55,  29,   55,  29,   55,  29,  55
										DW		74,  84,  74,   84,  74,   84,  74,  84
										DW		 74,  74,  74,   74,  74,   74,  74,  74
										DW		  0, -74,   0,  -74,   0,  -74,   0, -74
										DW		 84, -29,  84,  -29,  84,  -29,  84, -29
										DW		-74,  55, -74,   55, -74,   55, -74,  55
										DW		 55, -84,  55,  -84,  55,  -84,  55, -84
										DW		 74, -29,  74,  -29,  74,  -29,  74, -29

	dst_partial_bufferfly_inverse4_t4	DW		 29,  55,  29,   55,  29,   55,  29,  55
										DW		 55, -29,  55,  -29,  55,  -29,  55, -29
										DW		 74,   0,  74,    0,  74,    0,  74,   0
										DW		 55,  29,  55,   29,  55,   29,  55,  29
SECTION .text align=16

%macro LOAD_FOUR_INT16_T_SSSE3		2
	MOVQ %1, %2
%endmacro

%macro STORE_FOUR_INT16_T_SSSE3 2
	MOVQ %1, %2
%endmacro

%macro GET_XOFFSET_AND_XSHIFT_SSSE3				1
	MOV ecx, %1
	MOVD XSHIFT, ecx
	PSHUFB XSHIFT, [pshuffq_zero]
	MOV r0, 1
	DEC ecx
	SHL r0, cl
	MOVD XOFFSET, r0
	PSHUFB XOFFSET, [pshuffd_zero]
%endmacro

%macro MATRIX_TRANSPOSE_SRC_SSSE3 1
	%define				MTS_SRC					r1
	%define				MTS_SRC_STRIDE			r2
	MOV MTS_SRC, SRC
%if FIRST_ST == %1
	MOV MTS_SRC_STRIDE, r1m
%else
	MOV MTS_SRC_STRIDE, 4
%endif
	IMUL MTS_SRC_STRIDE, SIZE_OF_INT16_T

	LOAD_FOUR_INT16_T_SSSE3 XMMR0, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	LOAD_FOUR_INT16_T_SSSE3 XMMR1, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	LOAD_FOUR_INT16_T_SSSE3 XMMR2, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	LOAD_FOUR_INT16_T_SSSE3 XMMR3, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE

	PUNPCKLWD XMMR0, XMMR1
	PUNPCKLWD XMMR2, XMMR3
	PUNPCK 0, 2, 7, DQ

	MOVHLPS XMMR1, XMMR0
	MOVHLPS XMMR3, XMMR2
%endmacro

%macro CALCULATE_ALL_C_PARTIAL_BUFFER_FLY4_SSSE3				0
	MOVQ XMMR7, XMMR0
	PADDW XMMR7, XMMR3
	MOVQ [C+SIZE_OF_ONE_C*0], XMMR7

	MOVQ XMMR7, XMMR1
	PADDW XMMR7, XMMR3
	MOVQ [C+SIZE_OF_ONE_C*1], XMMR7

	MOVQ XMMR7, XMMR0
	PSUBW XMMR7, XMMR1
	MOVQ [C+SIZE_OF_ONE_C*2], XMMR7

	PXOR XMMR7, XMMR7
	PUNPCKLWD XMMR2, XMMR7
	PMADDWD XMMR2, [dst_partial_bufferfly_4_t4_1+16]
	MOVDQA [C+SIZE_OF_ONE_C*3], XMMR2
%endmacro

%macro			CALCULATE_COEFF0_PARTIAL_BUFFER_FLY4_SSSE3			0
	MOVQ XMMR7, [C+SIZE_OF_ONE_C*0]
	PUNPCKLWD XMMR7, [C+SIZE_OF_ONE_C*1]
	PMADDWD XMMR7, [dst_partial_bufferfly_4_t4_1]
	PADDD XMMR7, [C+SIZE_OF_ONE_C*3]

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	PSHUFB XMMR7, XSHUFFDW
	MOVQ [DST], XMMR7
%endmacro

%macro			CALCULATE_COEFF1_PARTIAL_BUFFER_FLY4_SSSE3			0
	PADDW XMMR0, XMMR1
	PSUBW XMMR0, XMMR3
	PXOR XMMR7, XMMR7
	PUNPCKLWD XMMR0, XMMR7
	PMADDWD XMMR0, [dst_partial_bufferfly_4_t4_1+16]

	PADDD XMMR0, XOFFSET
	PSRAD XMMR0, XSHIFT
	PSHUFB XMMR0, XSHUFFDW
	MOVQ [DST+LINE_SIZE_INT16_T*1], XMMR0
%endmacro

%macro			CALCULATE_COEFF2_PARTIAL_BUFFER_FLY4_SSSE3			0
	MOVQ XMMR7, [C+SIZE_OF_ONE_C*2]
	PUNPCKLWD XMMR7, [C+SIZE_OF_ONE_C*0]
	PMADDWD XMMR7, [dst_partial_bufferfly_4_t4_1+32]
	PSUBD XMMR7, [C+SIZE_OF_ONE_C*3]

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	PSHUFB XMMR7, XSHUFFDW
	MOVQ [DST+LINE_SIZE_INT16_T*2], XMMR7
%endmacro

%macro			CALCULATE_COEFF3_PARTIAL_BUFFER_FLY4_SSSE3			0
	MOVQ XMMR7, [C+SIZE_OF_ONE_C*2]
	PUNPCKLWD XMMR7, [C+SIZE_OF_ONE_C*1]
	PMADDWD XMMR7, [dst_partial_bufferfly_4_t4_1+48]
	PADDD XMMR7, [C+SIZE_OF_ONE_C*3]

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	PSHUFB XMMR7, XSHUFFDW
	MOVQ [DST+LINE_SIZE_INT16_T*3], XMMR7
%endmacro



%macro			CALCULATE_ALL_COEFF_PARTIAL_BUFFER_FLY4_SSSE3			0
	MOVDQA XSHUFFDW, [pshuffd_w]
	CALCULATE_COEFF1_PARTIAL_BUFFER_FLY4_SSSE3
	CALCULATE_COEFF0_PARTIAL_BUFFER_FLY4_SSSE3
	CALCULATE_COEFF2_PARTIAL_BUFFER_FLY4_SSSE3
	CALCULATE_COEFF3_PARTIAL_BUFFER_FLY4_SSSE3
%endmacro


%macro CALCULATE_ONE_COEFF_DCT_SSSE3		2
	MOVDQA XMMR7, XMMR0
	PMADDWD XMMR7, [%2]
	MOVDQA XMMR1, XMMR2
	PMADDWD XMMR1, [%2+16]
	PADDD XMMR7, XMMR1

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	MOVDQA %1, XMMR7
%endmacro

%macro			CALCULATE_ALL_COEFF_DCT_SSSE3					0
	CALCULATE_ONE_COEFF_DCT_SSSE3 [DST+LINE_SIZE_INT32_T*0], dst_partial_bufferfly_4_t4_2
	CALCULATE_ONE_COEFF_DCT_SSSE3 [DST+LINE_SIZE_INT32_T*1], dst_partial_bufferfly_4_t4_2+32
	CALCULATE_ONE_COEFF_DCT_SSSE3 [DST+LINE_SIZE_INT32_T*2], dst_partial_bufferfly_4_t4_2+64
	CALCULATE_ONE_COEFF_DCT_SSSE3 [DST+LINE_SIZE_INT32_T*3], dst_partial_bufferfly_4_t4_2+96
%endmacro

%macro			CALCULATE_ONE_LINE_DST_PARTIAL_BUFFER_FLY4_SSSE3				2
	MOV SRC, %1
	MATRIX_TRANSPOSE_SRC_SSSE3 FIRST_ST
	CALCULATE_ALL_C_PARTIAL_BUFFER_FLY4_SSSE3
	MOV DST, %2
	CALCULATE_ALL_COEFF_PARTIAL_BUFFER_FLY4_SSSE3
%endmacro

%macro			CALCULATE_ONE_LINE_DST_DCT_SSSE3				2
	MOV SRC, %1
	MATRIX_TRANSPOSE_SRC_SSSE3 SECOND_ND
	PUNPCKLWD XMMR0, XMMR1
	PUNPCKLWD XMMR2, XMMR3
	MOV DST, %2
	CALCULATE_ALL_COEFF_DCT_SSSE3
%endmacro

%macro X265_TR_QUANT_X_TR_DST_HELP_SSSE3		1
cglobal %1, 0, 7
	%define				SIZE_OF_ONE_C				 (4*4)
	%define				SIZE_OF_INT16_T				 2
	%define				SIZE_OF_INT32_T				 4
	%define				LINE_LENGTH					 4
	%define				LINE_SIZE_INT16_T			 (LINE_LENGTH*SIZE_OF_INT16_T)
	%define				LINE_SIZE_INT32_T			 (LINE_LENGTH*SIZE_OF_INT32_T)
	%define				NUM_OF_ALL_C				 4
	%define				SIZE_OF_ALL_C				 (NUM_OF_ALL_C*SIZE_OF_ONE_C)
	%define				TEMP_COEFF_SIZE				 (LINE_SIZE_INT16_T*LINE_LENGTH)
	%define				TEMP_COEFF					 r6
	%define				C							(TEMP_COEFF+TEMP_COEFF_SIZE)
	%define				SRC							r3
	%define				DST							r3
	%define				XSHUFFDW					XMMR4
	%define				XOFFSET						XMMR6
	%define				XSHIFT						XMMR5
	%define				FIRST_ST					1
	%define				SECOND_ND					2
	%define				SIZE_OF_STACK				(SIZE_OF_ALL_C+TEMP_COEFF_SIZE)

	mov r6, esp
	and r6, 0xFFFFFFF0
	sub r6, SIZE_OF_STACK

	MOV r2, r3m
	SUB r2, 7
	GET_XOFFSET_AND_XSHIFT_SSSE3 r2
	CALCULATE_ONE_LINE_DST_PARTIAL_BUFFER_FLY4_SSSE3 r0m, TEMP_COEFF

	MOV r2, 8
	GET_XOFFSET_AND_XSHIFT_SSSE3 r2
	CALCULATE_ONE_LINE_DST_DCT_SSSE3 TEMP_COEFF, r2m
	RET
%endmacro

%macro LOAD_TEMP_COEFF		1
%if FIRST_ST == %1
	MOVDQA XSHUFFDW, [pshuffd_w]
	MOVDQA XMMR0, [SRC+16*0]
	MOVDQA XMMR1, [SRC+16*1]
	MOVDQA XMMR2, [SRC+16*2]
	MOVDQA XMMR3, [SRC+16*3]
	PSHUFB XMMR0, XSHUFFDW
	PSHUFB XMMR1, XSHUFFDW
	PSHUFB XMMR2, XSHUFFDW
	PSHUFB XMMR3, XSHUFFDW
%else
	MOVQ XMMR0, [SRC+8*0]
	MOVQ XMMR1, [SRC+8*1]
	MOVQ XMMR2, [SRC+8*2]
	MOVQ XMMR3, [SRC+8*3]
%endif

%endmacro

%macro CALCULATE_ALL_C_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3				1
	LOAD_TEMP_COEFF %1
	MOVDQA XMMR7, XMMR0
	PADDW XMMR7, XMMR2
	MOVQ [C+SIZE_OF_ONE_C*0], XMMR7

	MOVDQA XMMR7, XMMR2
	PADDW XMMR7, XMMR3
	MOVQ [C+SIZE_OF_ONE_C*1], XMMR7

	MOVDQA XMMR7, XMMR0
	PSUBW XMMR7, XMMR3
	MOVQ [C+SIZE_OF_ONE_C*2], XMMR7

	PXOR XMMR7, XMMR7
	PUNPCKLWD XMMR1, XMMR7
	PMADDWD XMMR1, [dst_partial_bufferfly_inverse4_t4+16*2]
	MOVDQA [C+SIZE_OF_ONE_C*3], XMMR1
%endmacro

%macro			CALCULATE_COEFF0_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3			0
	MOVQ XMMR7, [C+SIZE_OF_ONE_C*0]
	PUNPCKLWD XMMR7, [C+SIZE_OF_ONE_C*1]
	PMADDWD XMMR7, [dst_partial_bufferfly_inverse4_t4+16*0]
	PADDD XMMR7, [C+SIZE_OF_ONE_C*3]

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	PSHUFB XMMR7, XSHUFFDW
	MOVQ XMMR0, XMMR7
%endmacro

%macro			CALCULATE_COEFF1_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3			0
	MOVQ XMMR7, [C+SIZE_OF_ONE_C*2]
	PUNPCKLWD XMMR7, [C+SIZE_OF_ONE_C*1]
	PMADDWD XMMR7, [dst_partial_bufferfly_inverse4_t4+16*1]
	PADDD XMMR7, [C+SIZE_OF_ONE_C*3]

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	PSHUFB XMMR7, XSHUFFDW
	MOVQ XMMR1, XMMR7
%endmacro

%macro			CALCULATE_COEFF2_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3			0
	PSUBW XMMR0, XMMR2
	PADDW XMMR0, XMMR3
	PXOR XMMR7, XMMR7
	PUNPCKLWD XMMR0, XMMR7
	PMADDWD XMMR0, [dst_partial_bufferfly_inverse4_t4+16*2]

	PADDD XMMR0, XOFFSET
	PSRAD XMMR0, XSHIFT
	PSHUFB XMMR0, XSHUFFDW
	MOVQ XMMR2, XMMR0
%endmacro


%macro			CALCULATE_COEFF3_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3			0
	MOVQ XMMR7, [C+SIZE_OF_ONE_C*0]
	PUNPCKLWD XMMR7, [C+SIZE_OF_ONE_C*2]
	PMADDWD XMMR7, [dst_partial_bufferfly_inverse4_t4+16*3]
	PSUBD XMMR7, [C+SIZE_OF_ONE_C*3]

	PADDD XMMR7, XOFFSET
	PSRAD XMMR7, XSHIFT
	PSHUFB XMMR7, XSHUFFDW
	MOVQ XMMR3, XMMR7
%endmacro

%macro			CALCULATE_ALL_COEFF_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3			1
	CALCULATE_COEFF2_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3
	CALCULATE_COEFF0_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3
	CALCULATE_COEFF1_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3
	CALCULATE_COEFF3_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3

	PUNPCKLWD XMMR0, XMMR1
	PUNPCKLWD XMMR2, XMMR3
	PUNPCK 0, 2, 7, DQ

%if FIRST_ST == %1
	MOVQ [DST], XMMR0
	MOVHLPS XMMR0, XMMR0
	MOVQ [DST+8], XMMR0
	MOVQ [DST+16], XMMR2
	MOVHLPS XMMR2, XMMR2
	MOVQ [DST+24], XMMR2
%else
	MOV r0, r1m
	IMUL r0, SIZE_OF_INT16_T

	MOVQ [DST], XMMR0
	ADD DST, r0
	MOVHLPS XMMR0, XMMR0
	MOVQ [DST], XMMR0
	ADD DST, r0

	MOVQ [DST], XMMR2
	ADD DST, r0
	MOVHLPS XMMR2, XMMR2
	MOVQ [DST], XMMR2
%endif
%endmacro

%macro CALCULATE_ONE_LINE_DST_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3		3
	MOV SRC, %1
	CALCULATE_ALL_C_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3		%3

	MOV DST, %2
	CALCULATE_ALL_COEFF_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3		%3

%endmacro


%macro X265_ITR_QUANT_X_TR_DST_HELP_SSSE3		1
cglobal %1, 0, 7
	%define				SIZE_OF_ONE_C				 (4*4)
	%define				SIZE_OF_INT16_T				 2
	%define				SIZE_OF_INT32_T				 4
	%define				LINE_LENGTH					 4
	%define				LINE_SIZE_INT16_T			 (LINE_LENGTH*SIZE_OF_INT16_T)
	%define				LINE_SIZE_INT32_T			 (LINE_LENGTH*SIZE_OF_INT32_T)
	%define				NUM_OF_ALL_C				 4
	%define				SIZE_OF_ALL_C				 (NUM_OF_ALL_C*SIZE_OF_ONE_C)
	%define				TEMP_BLOCK_SIZE				 (LINE_SIZE_INT16_T*LINE_LENGTH)
	%define				TEMP_BLOCK					 r6
	%define				C							(TEMP_BLOCK+TEMP_BLOCK_SIZE)
	%define				SRC							r3
	%define				DST							r3
	%define				XSHUFFDW					XMMR4
	%define				XOFFSET						XMMR6
	%define				XSHIFT						XMMR5
	%define				FIRST_ST					1
	%define				SECOND_ND					2
	%define				SIZE_OF_STACK				(SIZE_OF_ALL_C+TEMP_BLOCK_SIZE)

	mov r6, esp
	and r6, 0xFFFFFFF0
	sub r6, SIZE_OF_STACK

	MOV r2, 7
	GET_XOFFSET_AND_XSHIFT_SSSE3 r2
	CALCULATE_ONE_LINE_DST_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3 r2m, TEMP_BLOCK, FIRST_ST

	MOV r2, 20
	SUB r2, r3m
	GET_XOFFSET_AND_XSHIFT_SSSE3 r2
	CALCULATE_ONE_LINE_DST_PARTIAL_BUFFER_FLY_INVERSE4_SSSE3 TEMP_BLOCK, r0m, SECOND_ND

	RET
%endmacro


X265_TR_QUANT_X_TR_DST_HELP_SSSE3 tr_quant_x_tr_dst_ssse3
X265_ITR_QUANT_X_TR_DST_HELP_SSSE3 tr_quant_x_itr_dst_ssse3





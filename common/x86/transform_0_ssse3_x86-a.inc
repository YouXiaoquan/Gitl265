


%macro CLEAR_XMMR1_AND_XMMR2_SSSE3										0
	PXOR XMMR1, XMMR1
	PXOR XMMR2, XMMR2
%endmacro

%macro LOAD_16_BYTES_SSSE3		3
	%define			LOAD_INSTRUCTION		%3
	LOAD_INSTRUCTION %1, %2
	%undef			LOAD_INSTRUCTION
%endmacro

%macro STORE_16_BYTES_SSSE3		3
	%define			STORE_INSTRUCTION		%3
	STORE_INSTRUCTION %1, %2
	%undef			STORE_INSTRUCTION
%endmacro

%macro LOAD_FOUR_INT16_T_SSSE3		3
	MOVQ %1, %2
%endmacro

%macro STORE_EIGHT_INT16_T_SSSE3		3
	%define			STORE_INSTRUCTION		%3
	STORE_INSTRUCTION %1, %2
	%undef			STORE_INSTRUCTION
%endmacro

%macro LOAD_FOUR_INT32_T_SSSE3					4
	LOAD_16_BYTES_SSSE3 %1, [%2], %4
	PSHUFB %1, XSHUFFDW
%endmacro

%macro STORE_EIGHT_INT32_T_SSSE3					4
	STORE_16_BYTES_SSSE3 [%3+K   ], %1, %4
	STORE_16_BYTES_SSSE3 [%3+K+16], %2, %4
%endmacro

%macro MATRIX_TRANSPOSE_SSSE3			6
	%define				MT_DST					%1
	%define				MT_DST_STRIDE			%2
	%define				MT_SRC					%3
	%define				MT_SRC_STRIDE			%4
	%define				MT_R0					[MT_SRC+MT_SRC_STRIDE*0]
	%define				MT_R1					[MT_SRC+MT_SRC_STRIDE*1]
	%define				MT_R2					[MT_SRC+MT_SRC_STRIDE*2]
	%define				MT_R3					[MT_SRC+MT_SRC_STRIDE*3]

	MOVQ XMMR0, MT_R0
	MOVQ XMMR1, MT_R1
	MOVQ XMMR2, MT_R2
	MOVQ XMMR3, MT_R3

	PUNPCKLWD XMMR0, XMMR1
	PUNPCKLWD XMMR2, XMMR3

	PUNPCK 0, 2, 7, DQ

	MOVQ [MT_DST+MT_DST_STRIDE*0], XMMR0
	MOVHLPS XMMR0, XMMR0
	MOVQ [MT_DST+MT_DST_STRIDE*1], XMMR0

	MOVQ [MT_DST+MT_DST_STRIDE*2], XMMR2
	MOVHLPS XMMR2, XMMR2
	MOVQ [MT_DST+MT_DST_STRIDE*3], XMMR2

	%undef				MT_R0
	%undef				MT_R1
	%undef				MT_R2
	%undef				MT_R3
%endmacro

%macro MATRIX_TRANSPOSE_SRC_SSSE3 3
	%define				MTS_DST_STRIDE			(SIZE_OF_INT16_T*8)
	%define				MTS_DST					(TEMP_SRC+%2*MTS_DST_STRIDE)
	%define				MTS_SRC					r1
	%define				MTS_SRC_STRIDE			r2
	MOV MTS_SRC, SRC
	MOV MTS_SRC_STRIDE, r1m
	IMUL MTS_SRC_STRIDE, SIZE_OF_INT16_T

	MOVQ XMMR0, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	MOVQ XMMR1, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	PUNPCKLWD XMMR0, XMMR1
	
	MOVQ XMMR2, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	MOVQ XMMR3, [MTS_SRC]
	ADD MTS_SRC, MTS_SRC_STRIDE
	PUNPCKLWD XMMR2, XMMR3
	PUNPCK 0, 2, 7, DQ

	
	MOVQ [MTS_DST+MTS_DST_STRIDE*0], XMMR0
	MOVHLPS XMMR0, XMMR0
	MOVQ [MTS_DST+MTS_DST_STRIDE*1], XMMR0
	
	MOVQ [MTS_DST+MTS_DST_STRIDE*2], XMMR2
	MOVHLPS XMMR2, XMMR2
	MOVQ [MTS_DST+MTS_DST_STRIDE*3], XMMR2
		
%endmacro

%macro MATRIX_TRANSPOSE_TMP_COEFF_SSSE3 1
	%define				MTTC_DST				(DST+%1*SIZE_OF_INT16_T)
	%define				MTTC_DST_STRIDE			(LINE_LENGTH * SIZE_OF_INT16_T)
	%define				MTTC_SRC				(TEMP_SRC+%1*MTTC_SRC_STRIDE)
	%define				MTTC_SRC_STRIDE			(SIZE_OF_INT16_T*8)
	MATRIX_TRANSPOSE_SSSE3 MTTC_DST, MTTC_DST_STRIDE, MTTC_SRC, MTTC_SRC_STRIDE, MOVDQA, MOVDQA
%endmacro


%macro MATRIX_TRANSPOSE_AND_SAVE_TO_DST_SSSE3 3
%if FIRST_ST == %2
	%define				MTASTD_DST					(DST + SIZE_OF_INT16_T*%1)
	MATRIX_TRANSPOSE_SSSE3 MTASTD_DST, LINE_SIZE_INT16_T, TEMP_DST, SIZE_OF_ONE_EO, MOVDQA, %3
%else
	%define				MTASTD_DST					r1
	%define				MTASTD_DST_STRIDE			r2
	%define				MTASTD_SRC					TEMP_DST
	%define				MTASTD_SRC_STRIDE			SIZE_OF_ONE_EO
	%define				MTASTD_R0					[MTASTD_SRC+MTASTD_SRC_STRIDE*0]
	%define				MTASTD_R1					[MTASTD_SRC+MTASTD_SRC_STRIDE*1]
	%define				MTASTD_R2					[MTASTD_SRC+MTASTD_SRC_STRIDE*2]
	%define				MTASTD_R3					[MTASTD_SRC+MTASTD_SRC_STRIDE*3]
	%define				MTASTD_R4					[MTASTD_SRC+MTASTD_SRC_STRIDE*4]
	%define				MTASTD_R5					[MTASTD_SRC+MTASTD_SRC_STRIDE*5]
	%define				MTASTD_R6					[MTASTD_SRC+MTASTD_SRC_STRIDE*6]
	%define				MTASTD_R7					[MTASTD_SRC+MTASTD_SRC_STRIDE*7]
	MOV MTASTD_DST, SIZE_OF_INT16_T*%1
	ADD MTASTD_DST, DST
	MOV MTASTD_DST_STRIDE, r1m
	IMUL MTASTD_DST_STRIDE, SIZE_OF_INT16_T
	LOAD_EIGHT_INT16_T_SSSE3 XMMR0, MTASTD_R0, MOVDQA
	MOVDQA XMMR1, XMMR0
	LOAD_EIGHT_INT16_T_SSSE3 XMMR7, MTASTD_R1, MOVDQA
	PUNPCKLWD XMMR0, XMMR7
	PUNPCKHWD XMMR1, XMMR7
	LOAD_EIGHT_INT16_T_SSSE3 XMMR2, MTASTD_R2, MOVDQA
	MOVDQA XMMR3, XMMR2
	LOAD_EIGHT_INT16_T_SSSE3 XMMR7, MTASTD_R3, MOVDQA
	PUNPCKLWD XMMR2, XMMR7
	PUNPCKHWD XMMR3, XMMR7
	PUNPCK 0, 2, 7, DQ
	PUNPCK 1, 3, 7, DQ
	STORE_EIGHT_INT16_T_SSSE3 [MTB1], XMMR1, %3
	STORE_EIGHT_INT16_T_SSSE3 [MTB2], XMMR2, %3
	STORE_EIGHT_INT16_T_SSSE3 [MTB3], XMMR3, %3
	MY_SWAP 1, 5
	MY_SWAP 2, 6
	LOAD_EIGHT_INT16_T_SSSE3 XMMR4, MTASTD_R4, MOVDQA
	MOVDQA XMMR5, XMMR4
	LOAD_EIGHT_INT16_T_SSSE3 XMMR3, MTASTD_R5, MOVDQA
	PUNPCKLWD XMMR4, XMMR3
	PUNPCKHWD XMMR5, XMMR3
	LOAD_EIGHT_INT16_T_SSSE3 XMMR6, MTASTD_R6, MOVDQA
	MOVDQA XMMR7, XMMR6
	LOAD_EIGHT_INT16_T_SSSE3 XMMR3, MTASTD_R7, MOVDQA
	PUNPCKLWD XMMR6, XMMR3
	PUNPCKHWD XMMR7, XMMR3
	PUNPCK 4, 6, 3, DQ
	PUNPCK 5, 7, 3, DQ
	PUNPCK 0, 4, 3, QDQ
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR0, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR4, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	LDDQU XMMR0, [MTB2]
	PUNPCK 0, 6, 3, QDQ
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR0, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR6, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	LDDQU XMMR0, [MTB1]
	PUNPCK 0, 5, 3, QDQ
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR0, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR5, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	LDDQU XMMR0, [MTB3]
	PUNPCK 0, 7, 3, QDQ
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR0, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	STORE_EIGHT_INT16_T_SSSE3 [MTASTD_DST], XMMR7, %3
	ADD MTASTD_DST, MTASTD_DST_STRIDE
	MY_SWAP 1, 5
	MY_SWAP 2, 6
	%undef				MTASTD_R0
	%undef				MTASTD_R1
	%undef				MTASTD_R2
	%undef				MTASTD_R3
	%undef				MTASTD_R4
	%undef				MTASTD_R5
	%undef				MTASTD_R6
	%undef				MTASTD_R7
%endif
%endmacro

%macro GET_XOFFSET_AND_XSHIFT_SSSE3				1
	MOV ecx, %1
	MOVD XSHIFT, ecx
	PSHUFB XSHIFT, [pshuffq_zero]
	MOV r0, 1
	DEC ecx
	SHL r0, cl
	MOVD XOFFSET, r0
	PSHUFB XOFFSET, [pshuffd_zero]
%endmacro

%macro MUL_AND_ADD_ONE_LINE_NUM_PARTIAL_BUFFER_FLY_SSSE3		4
	MOVDQA XSPACE1, [%3+(%1*SIZE_OF_ONE_EO)]
	MOVDQA XSPACE3, [%3+(%2*SIZE_OF_ONE_EO)]
	MOVDQA XSPACE2, XSPACE1
	PUNPCKLWD XSPACE1, XSPACE3
	PUNPCKHWD XSPACE2, XSPACE3

	PMADDWD XSPACE1, [TCOEFF+(%4*SIZE_OF_ONE_EO)]
	PMADDWD XSPACE2, [TCOEFF+(%4*SIZE_OF_ONE_EO)]
	PADDD XMMR1, XSPACE1
	PADDD XMMR2, XSPACE2
%endmacro

%macro MUL_AND_ADD_ONE_LINE_NUM_PARTIAL_BUFFER_FLY_INVERSE_SSSE3		5
%if SECOND_ND == %4
	;LOAD_EIGHT_INT16_T_SSSE3 XSPACE1, [SRC+LINE_SIZE_INT16_T*%1], %5
	;LOAD_EIGHT_INT16_T_SSSE3 XSPACE3, [SRC+LINE_SIZE_INT16_T*%2], %5
%else
	;LOAD_EIGHT_INT32_T_SSSE3 XSPACE1, (SRC+LINE_SIZE_INT32_T*%1), XSPACE2, %5
	;LOAD_EIGHT_INT32_T_SSSE3 XSPACE3, (SRC+LINE_SIZE_INT32_T*%2), XSPACE2, %5
%endif
	MOVDQA XSPACE2, XSPACE1
	PUNPCKLWD XSPACE1, XSPACE3
	PUNPCKHWD XSPACE2, XSPACE3

	PMADDWD XSPACE1, [TCOEFF+%1*SIZE_OF_ONE_EO*%3]
	PMADDWD XSPACE2, [TCOEFF+%1*SIZE_OF_ONE_EO*%3]
	PADDD XMMR1, XSPACE1
	PADDD XMMR2, XSPACE2
%endmacro

%macro			CALCULATE_ONE_E_O_PARTIAL_BUFFER_FLY_SSSE3			4
	MOVDQA XMMR1, [%3+K]
	MOVDQA XMMR2, XMMR1
	MOVDQA XSPACE1, [%3+r0]
	PADDW XMMR1, XSPACE1
	PSUBW XMMR2, XSPACE1
	MOVDQA [%1+%4+K], XMMR1
	MOVDQA [%2+%4+K], XMMR2
%endmacro

%macro ADD_OFFSET_AND_SHIFT_XMMR1_AND_XMMR2_SSSE3 0
	PADDD XMMR1, XOFFSET
	PSRAD XMMR1, XSHIFT
	PSHUFB XMMR1, XSHUFFDW
	PADDD XMMR2, XOFFSET
	PSRAD XMMR2, XSHIFT
	PSHUFB XMMR2, XSHUFFDW
	PUNPCKLQDQ XMMR1, XMMR2
%endmacro

%macro			CALCULATE_ONE_E_PARTIAL_BUFFER_FLY_INVERSE_SSSE3		4
	MOVDQA XMMR1, [%1+K]
	MOVDQA XMMR2, [%2+K]
	PADDD XMMR1, XMMR2
	MOVDQA [%3+K                   ], XMMR1
	MOVDQA XMMR1, [%1+K+16]
	MOVDQA XMMR2, [%2+K+16]
	PADDD XMMR1, XMMR2
	MOVDQA [%3+K+16                ], XMMR1

	MOVDQA XMMR1, [%1+r0]
	MOVDQA XMMR2, [%2+r0]
	PSUBD XMMR1, XMMR2
	MOVDQA [%3+SIZE_OF_ONE_EO*%4+K   ], XMMR1
	MOVDQA XMMR1, [%1+r0+16]
	MOVDQA XMMR2, [%2+r0+16]
	PSUBD XMMR1, XMMR2
	MOVDQA [%3+SIZE_OF_ONE_EO*%4+K+16], XMMR1
%endmacro

%macro			CALCULATE_E_PARTIAL_BUFFER_FLY_INVERSE_SSSE3			4
	MOV K, 0
	MOV r0, SIZE_OF_ONE_EO*(%4-1)
%%CALCULATE_E_PARTIAL_BUFFER_FLY_INVERSE_SSSE3_LABEL_K:
	CALCULATE_ONE_E_PARTIAL_BUFFER_FLY_INVERSE_SSSE3 %1, %2, %3, %4

	ADD K, SIZE_OF_ONE_EO
	SUB r0, SIZE_OF_ONE_EO
	CMP K, SIZE_OF_ONE_EO * %4
	JL %%CALCULATE_E_PARTIAL_BUFFER_FLY_INVERSE_SSSE3_LABEL_K
%endmacro

%macro			CALCULATE_ONE_PARTIAL_BUFFER_FLY_INVERSE_SSSE3		2
	MOVDQA XMMR1, [E+EO_ADDRESS]
	MOVDQA XMMR2, [E+EO_ADDRESS+16]
%if %2 == FORWARD
	PADDD XMMR1, [O+EO_ADDRESS]
	PADDD XMMR2, [O+EO_ADDRESS+16]
%else
	PSUBD XMMR1, [O+EO_ADDRESS]
	PSUBD XMMR2, [O+EO_ADDRESS+16]
%endif
	PADDD XMMR1, XOFFSET
	PSRAD XMMR1, XSHIFT
	PSHUFB XMMR1, [pshuffd_w]
	PADDD XMMR2, XOFFSET
	PSRAD XMMR2, XSHIFT
	PSHUFB XMMR2, [pshuffd_w]
	PUNPCKLQDQ XMMR1, XMMR2
	MOVDQA [TEMP_DST+K], XMMR1
%endmacro

%macro			CALCULATE_ONE_PARTIAL_BUFFER_FLY_INVERSE8_SSSE3		2
	MOVDQA %1, [E+EO_ADDRESS1]
	MOVDQA %2, [E+EO_ADDRESS2]
	PADDD %1, [O+EO_ADDRESS1]
	PSUBD %2, [O+EO_ADDRESS2]
	PADDD %1, XOFFSET
	PADDD %2, XOFFSET
	PSRAD %1, XSHIFT
	PSRAD %2, XSHIFT
	PSHUFB %1, XSHUFFDW
	PSHUFB %2, XSHUFFDW
	ADD EO_ADDRESS1, SIZE_OF_ONE_EO
	SUB EO_ADDRESS2, SIZE_OF_ONE_EO

	MOVDQA XSPACE1, [E+EO_ADDRESS1]
	PADDD XSPACE1, [O+EO_ADDRESS1]
	PADDD XSPACE1, XOFFSET
	PSRAD XSPACE1, XSHIFT
	PSHUFB XSPACE1, XSHUFFDW
	PUNPCKLWD %1, XSPACE1
	
	MOVDQA XSPACE1, [E+EO_ADDRESS2]
	PSUBD XSPACE1, [O+EO_ADDRESS2]
	PADDD XSPACE1, XOFFSET
	PSRAD XSPACE1, XSHIFT
	PSHUFB XSPACE1, XSHUFFDW
	PUNPCKLWD %2, XSPACE1

	
%endmacro

